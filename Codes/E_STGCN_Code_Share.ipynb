{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt5qWoBIc6sR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import typing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "station_distances = np.array(pd.read_csv(\"file path for Haversine distance file\"))\n",
        "aqi = pd.DataFrame(pd.read_csv(\"file path for Daily dataset\"))\n",
        "print(station_distances.shape)\n",
        "pot=pd.read_csv(\"file path POT estimates for each station\")\n",
        "pot_est=pd.DataFrame(pot)\n",
        "pot_results=np.array(pot_est)[:,1:]\n",
        "\n",
        "aqi_array=aqi.to_numpy()\n",
        "print(aqi_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jan= 1460 # no of obervations training + validation"
      ],
      "metadata": {
        "id": "nE-6_dg3jI1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GPeWFVpgOdT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess(data_array: np.ndarray):\n",
        "    \"\"\"Splits data into train/val/test sets and normalizes the data.\n",
        "\n",
        "    Args:\n",
        "        data_array: ndarray of shape `(num_time_steps, num_routes)`\n",
        "        train_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n",
        "            to include in the train split.\n",
        "        val_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n",
        "            to include in the validation split.\n",
        "\n",
        "    Returns:\n",
        "        `train_array`, `val_array`, `test_array`\n",
        "    \"\"\"\n",
        "\n",
        "    num_time_steps = data_array.shape[0]\n",
        "    num_train, num_val = (\n",
        "        (jan-180),\n",
        "        180, # 180 is the no. of validation\n",
        "    )\n",
        "    train_array = data_array[:num_train]\n",
        "    mean, std = train_array.mean(axis=0), train_array.std(axis=0)\n",
        "\n",
        "    train_array = ((train_array - mean) / std)\n",
        "    val_array = ((data_array[num_train : (num_train + num_val)] - mean) / std)\n",
        "    #test_array = ((data_array[(num_train + num_val) :(num_train + num_val+30)] - mean) / std)\n",
        "    mean_val = mean\n",
        "    std_val = std\n",
        "\n",
        "    return train_array, val_array, mean_val, std_val\n",
        "\n",
        "\n",
        "train_array, val_array, mean_val_1, std_val_1 = preprocess(aqi_array)\n",
        "\n",
        "print(f\"train set size: {train_array.shape}\")\n",
        "print(f\"validation set size: {val_array.shape}\")\n",
        "#print(f\"test set size: {test_array.shape}\")\n",
        "n_stations = train_array.shape[1]\n",
        "\n",
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "\n",
        "batch_size = 64\n",
        "input_sequence_length = 24\n",
        "forecast_horizon = 90\n",
        "multi_horizon = False\n",
        "\n",
        "\n",
        "def create_tf_dataset(\n",
        "    data_array: np.ndarray,\n",
        "    input_sequence_length: int,\n",
        "    forecast_horizon: int,\n",
        "    batch_size: int =64,\n",
        "    shuffle=True,\n",
        "    multi_horizon=True,\n",
        "):\n",
        "\n",
        "    inputs = timeseries_dataset_from_array(\n",
        "        np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
        "        None,\n",
        "        sequence_length=input_sequence_length,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    target_offset = (\n",
        "        input_sequence_length\n",
        "        if multi_horizon\n",
        "        else input_sequence_length + forecast_horizon - 1\n",
        "    )\n",
        "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
        "    targets = timeseries_dataset_from_array(\n",
        "        data_array[target_offset:],\n",
        "        None,\n",
        "        sequence_length=target_seq_length,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(100)\n",
        "\n",
        "    return dataset.prefetch(16).cache()\n",
        "\n",
        "train_dataset, val_dataset = (\n",
        "    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
        "    for data_array in [train_array, val_array]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKDWnbtXiQBF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "\n",
        "batch_size = 64\n",
        "input_sequence_length = 24\n",
        "forecast_horizon = 90\n",
        "multi_horizon = False\n",
        "\n",
        "\n",
        "def create_tf_dataset(\n",
        "    data_array: np.ndarray,\n",
        "    input_sequence_length: int,\n",
        "    forecast_horizon: int,\n",
        "    batch_size: int =100,\n",
        "    shuffle=True,\n",
        "    multi_horizon=True,\n",
        "):\n",
        "    \"\"\"Creates tensorflow dataset from numpy array.\n",
        "\n",
        "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
        "    `inputs` is a Tensor\n",
        "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
        "    the `input_sequence_length` past values of the timeseries for each node.\n",
        "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
        "    containing the `forecast_horizon`\n",
        "    future values of the timeseries for each node.\n",
        "\n",
        "    Args:\n",
        "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
        "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
        "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
        "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
        "            timeseries `forecast_horizon` steps ahead (only one value).\n",
        "        batch_size: Number of timeseries samples in each batch.\n",
        "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
        "        multi_horizon: See `forecast_horizon`.\n",
        "\n",
        "    Returns:\n",
        "        A tf.data.Dataset instance.\n",
        "    \"\"\"\n",
        "# Print the shape of the data array before slicing to debug\n",
        "    print(\"Shape of data_array before slicing:\", data_array.shape)\n",
        "\n",
        "    inputs = timeseries_dataset_from_array(\n",
        "        np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
        "        None,\n",
        "        sequence_length=input_sequence_length,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    target_offset = (\n",
        "        input_sequence_length\n",
        "        if multi_horizon\n",
        "        else input_sequence_length + forecast_horizon - 1\n",
        "    )\n",
        "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
        "    targets = timeseries_dataset_from_array(\n",
        "        data_array[target_offset:],\n",
        "        None,\n",
        "        sequence_length=target_seq_length,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(100)\n",
        "\n",
        "    return dataset.prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = (\n",
        "    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
        "    for data_array in [train_array, val_array]\n",
        ")\n",
        "\n",
        "#test_dataset = create_tf_dataset(\n",
        "#    test_array,\n",
        "#    input_sequence_length,\n",
        "#    forecast_horizon,\n",
        "#    batch_size=test_array.shape[0],\n",
        "#    shuffle=False,\n",
        "#    multi_horizon=multi_horizon,\n",
        "#)\n",
        "\n",
        "def compute_adjacency_matrix(\n",
        "    route_distances: np.ndarray, sigma2: float, epsilon: float\n",
        "):\n",
        "    \"\"\"Computes the adjacency matrix from distances matrix.\n",
        "\n",
        "    It uses the formula in https://github.com/VeritasYin/STGCN_IJCAI-18#data-preprocessing to\n",
        "    compute an adjacency matrix from the distance matrix.\n",
        "    The implementation follows that paper.\n",
        "\n",
        "    Args:\n",
        "        route_distances: np.ndarray of shape `(num_routes, num_routes)`. Entry `i,j` of this array is the\n",
        "            distance between roads `i,j`.\n",
        "        sigma2: Determines the width of the Gaussian kernel applied to the square distances matrix.\n",
        "        epsilon: A threshold specifying if there is an edge between two nodes. Specifically, `A[i,j]=1`\n",
        "            if `np.exp(-w2[i,j] / sigma2) >= epsilon` and `A[i,j]=0` otherwise, where `A` is the adjacency\n",
        "            matrix and `w2=route_distances * route_distances`\n",
        "\n",
        "    Returns:\n",
        "        A boolean graph adjacency matrix.\n",
        "    \"\"\"\n",
        "    num_stations = 37 # No of locations in the dataset\n",
        "    station_distances_norm = station_distances / 10000.0\n",
        "    w2, w_mask = (\n",
        "        station_distances_norm * station_distances_norm,\n",
        "        np.ones([num_stations, num_stations]) - np.identity(num_stations),\n",
        "    )\n",
        "    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask\n",
        "\n",
        "class GraphInfo:\n",
        "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
        "        self.edges = edges\n",
        "        self.num_nodes = num_nodes\n",
        "\n",
        "\n",
        "sigma2 = 0.75\n",
        "epsilon = 0.5\n",
        "adjacency_matrix = compute_adjacency_matrix(station_distances, sigma2, epsilon)\n",
        "node_indices, neighbor_indices = np.where(adjacency_matrix == 1)\n",
        "graph = GraphInfo(\n",
        "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
        "    num_nodes=adjacency_matrix.shape[0],\n",
        ")\n",
        "print(f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If1LsuQsvw7p"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphConv(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feat,\n",
        "        out_feat,\n",
        "        graph_info: GraphInfo,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        activation: typing.Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.graph_info = graph_info\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.weight = tf.Variable(\n",
        "            initial_value=keras.initializers.glorot_uniform()(\n",
        "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
        "            ),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.activation = layers.Activation(activation)\n",
        "\n",
        "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
        "        aggregation_func = {\n",
        "            \"sum\": tf.math.unsorted_segment_sum,\n",
        "            \"mean\": tf.math.unsorted_segment_mean,\n",
        "            \"max\": tf.math.unsorted_segment_max,\n",
        "        }.get(self.aggregation_type)\n",
        "\n",
        "        if aggregation_func:\n",
        "            return aggregation_func(\n",
        "                neighbour_representations,\n",
        "                self.graph_info.edges[0],\n",
        "                num_segments=self.graph_info.num_nodes,\n",
        "            )\n",
        "\n",
        "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
        "\n",
        "    def compute_nodes_representation(self, features: tf.Tensor):\n",
        "        \"\"\"Computes each node's representation.\n",
        "\n",
        "        The nodes' representations are obtained by multiplying the features tensor with\n",
        "        `self.weight`. Note that\n",
        "        `self.weight` has shape `(in_feat, out_feat)`.\n",
        "\n",
        "        Args:\n",
        "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
        "        \"\"\"\n",
        "        return tf.matmul(features, self.weight)\n",
        "\n",
        "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
        "        neighbour_representations = tf.gather(features, self.graph_info.edges[1])\n",
        "        aggregated_messages = self.aggregate(neighbour_representations)\n",
        "        return tf.matmul(aggregated_messages, self.weight)\n",
        "\n",
        "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
        "        if self.combination_type == \"concat\":\n",
        "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            h = nodes_representation + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        return self.activation(h)\n",
        "\n",
        "    def call(self, features: tf.Tensor):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
        "        \"\"\"\n",
        "        nodes_representation = self.compute_nodes_representation(features)\n",
        "        aggregated_messages = self.compute_aggregated_messages(features)\n",
        "        return self.update(nodes_representation, aggregated_messages)\n",
        "\n",
        "class LSTMGC(layers.Layer):\n",
        "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feat,\n",
        "        out_feat,\n",
        "        lstm_units: int,\n",
        "        input_seq_len: int,\n",
        "        output_seq_len: int,\n",
        "        graph_info: GraphInfo,\n",
        "        graph_conv_params: typing.Optional[dict] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # graph conv layer\n",
        "        if graph_conv_params is None:\n",
        "            graph_conv_params = {\n",
        "                \"aggregation_type\": \"mean\",\n",
        "                \"combination_type\": \"concat\",\n",
        "                \"activation\": None,\n",
        "            }\n",
        "        self.graph_conv = GraphConv(in_feat, out_feat, graph_info, **graph_conv_params)\n",
        "\n",
        "        self.lstm = layers.LSTM(lstm_units, activation=\"relu\")\n",
        "        self.dense = layers.Dense(output_seq_len)\n",
        "\n",
        "        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
        "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
        "\n",
        "        gcn_out = self.graph_conv(\n",
        "            inputs\n",
        "        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n",
        "        shape = tf.shape(gcn_out)\n",
        "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
        "            shape[0],\n",
        "            shape[1],\n",
        "            shape[2],\n",
        "            shape[3],\n",
        "        )\n",
        "\n",
        "        # LSTM takes only 3D tensors as input\n",
        "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
        "        lstm_out = self.lstm(\n",
        "            gcn_out\n",
        "        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
        "\n",
        "        dense_output = self.dense(\n",
        "            lstm_out\n",
        "        )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n",
        "        output = tf.reshape(dense_output, (num_nodes, batch_size, self.output_seq_len))\n",
        "        return tf.transpose(\n",
        "            output, [1, 2, 0]\n",
        "        )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzK2rTOtyBO1"
      },
      "outputs": [],
      "source": [
        "##PoT and GPD\n",
        "H=90 # Forecast horizon\n",
        "nds=37 # no of locations\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "t_array = aqi_array[:(jan),:]\n",
        "sigma=pot_results[:,1]\n",
        "xi=pot_results[:,0]\n",
        "a=np.mean(t_array,axis=0)\n",
        "b=np.std(t_array,axis=0)\n",
        "A=np.zeros([H,nds])\n",
        "for i in range(0,H):\n",
        "  A[i,:]=a\n",
        "B=np.zeros([H,nds])\n",
        "for i in range(0,H):\n",
        "  B[i,:]=b\n",
        "Sig=np.zeros([H,nds])\n",
        "for i in range(0,H):\n",
        "  Sig[i,:]=sigma\n",
        "Xi=np.zeros([H,nds])\n",
        "for i in range(0,H):\n",
        "  Xi[i,:]=xi\n",
        "\n",
        "A=K.constant(A)\n",
        "B=K.constant(B)\n",
        "Xi=K.constant(Xi)\n",
        "Sig=K.constant(Sig)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    import tensorflow as tf\n",
        "    import numpy as np  # Make sure np is imported\n",
        "    import math\n",
        "\n",
        "    # Assume B, Sig, Xi, A, H, nds, K are defined globally and are tf constants\n",
        "\n",
        "    L1 = tf.math.log(B)\n",
        "    L2 = tf.math.log(Sig)\n",
        "    L3 = tf.math.add(1.0, tf.math.divide_no_nan(1.0, Xi))\n",
        "    L4 = tf.math.divide(tf.math.multiply(Xi, tf.math.add(A, tf.math.multiply(B, y_pred[0, :, :]))), Sig)\n",
        "    L5 = tf.math.subtract(L1, L2)\n",
        "    L6 = tf.math.add(1.0, L4)\n",
        "    L7 = tf.math.multiply(L3, tf.math.log(L6))\n",
        "    L = tf.math.subtract(L5, L7)  # GPD-style loss term\n",
        "\n",
        "    # Conditioning step: apply GPD loss only when y_pred exceeds threshold\n",
        "    threshold_mask = tf.math.greater(\n",
        "        tf.math.add(A, tf.math.multiply(B, y_pred[0, :, :])),\n",
        "        tf.constant(60.0) * tf.ones([H, nds]) # 60 is the threshold here\n",
        "    )\n",
        "    LK = tf.where(threshold_mask, L, tf.zeros([H, nds]))\n",
        "    beta_1 =  0.99\n",
        "    beta_2 = 0.01 # Tune these values as needed\n",
        "\n",
        "    # Compute loss terms\n",
        "    squared_difference = tf.square(y_true[0, :, :] - y_pred[0, :, :])\n",
        "    penalty_term = -LK[:, :]  # Negative log-likelihood (conditioned)\n",
        "\n",
        "    # Combine losses\n",
        "    return beta_1 * tf.reduce_mean(squared_difference, axis=-1) + \\\n",
        "           beta_2 * tf.reduce_max(penalty_term, axis=-1)\n"
      ],
      "metadata": {
        "id": "ObcKM9Tkv4Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "YBu9QrEgBm24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41h01ETMv7lb"
      },
      "outputs": [],
      "source": [
        "in_feat = 1\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "input_sequence_length = 24\n",
        "forecast_horizon = 90\n",
        "multi_horizon = False\n",
        "out_feat = 10\n",
        "lstm_units = 64\n",
        "graph_conv_params = {\n",
        "    \"aggregation_type\": \"mean\",\n",
        "    \"combination_type\": \"concat\",\n",
        "    \"activation\": None,\n",
        "}\n",
        "\n",
        "st_gcn = LSTMGC(\n",
        "    in_feat,\n",
        "    out_feat,\n",
        "    lstm_units,\n",
        "    input_sequence_length,\n",
        "    forecast_horizon,\n",
        "    graph,\n",
        "    graph_conv_params,\n",
        ")\n",
        "inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n",
        "outputs = st_gcn(inputs)\n",
        "\n",
        "model = keras.models.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n",
        "    loss=loss_fn, #keras.losses.MeanSquaredError(),\n",
        ")\n",
        "start = time.time()\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
        ")\n",
        "end = time.time()\n",
        "print(f\"Execution time: {end - start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNNOnRN4P1nJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "x_val, y = next(val_dataset.as_numpy_iterator())\n",
        "\n",
        "y_pred=model.predict(x_val)\n",
        "end = time.time()\n",
        "print(f\"Execution time: {end - start} seconds\")\n",
        "\n",
        "y_pred.shape\n",
        "pred=y_pred[0,:,:]\n",
        "\n",
        "df_1st = pd.DataFrame(pred)\n",
        "df_new_1 = (df_1st * std_val_1) + mean_val_1\n",
        "df_new_1 # Final Forecast"
      ]
    }
  ]
}